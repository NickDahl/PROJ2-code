---
title: "PROJ2"
author: "Emma Schmidt"
date: "11/13/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(GGally)
library(ggplot2)
library(corrplot)
library(hrbrthemes)
library(gridExtra)
library(dplyr)
library(caret)
library(ROCR)
library(e1071)
library(nnet)
library(plotROC)
library(pROC)
```


```{r}
# Read in the image data sets
x1 <- read.table("imagem1.txt")
x2 <- read.table("imagem2.txt")
x3 <- read.table("imagem3.txt")

colnames(x1) <- colnames(x2) <- colnames(x3) <- 
  c("ycoord", "xcoord", "label", "NDAI", "SD", "CORR", 
    "DF", "CF", "BF", "AF", "AN")
```

```{r}
# Create subsamples for exploratory plots
x1t <- x1[sample(nrow(x1), 10000), ]
x2t <- x2[sample(nrow(x2), 10000), ]
x3t <- x3[sample(nrow(x3), 10000), ]
```


Part 1b: Summarize the data

```{r}
# % of pixels for different classes by image
print(paste0("Percentage of image 1 that is cloudy: ", 
             round(sum(x1$label == 1) / nrow(x1) * 100, 2), "%"))
print(paste0("Percentage of image 1 that is not cloudy: ", 
             round(sum(x1$label == -1) / nrow(x1) * 100, 2), "%"))
print(paste0("Percentage of image 1 that is unlabeled: ", 
             round(sum(x1$label == 0) / nrow(x1) * 100, 2), "%"))


print(paste0("Percentage of image 2 that is cloudy: ", 
             round(sum(x2$label == 1) / nrow(x1) * 100, 2), "%"))
print(paste0("Percentage of image 2 that is not cloudy: ", 
             round(sum(x2$label == -1) / nrow(x1) * 100, 2), "%"))
print(paste0("Percentage of image 2 that is unlabeled: ", 
             round(sum(x2$label == 0) / nrow(x1) * 100, 2), "%"))


print(paste0("Percentage of image 3 that is cloudy: ", 
             round(sum(x3$label == 1) / nrow(x1) * 100, 2), "%"))
print(paste0("Percentage of image 3 that is not cloudy: ", 
             round(sum(x3$label == -1) / nrow(x1) * 100, 2), "%"))
print(paste0("Percentage of image 3 that is unlabeled: ", 
             round(sum(x3$label == 0) / nrow(x1) * 100, 2), "%"))

# General summary plots
ggpairs(x1[1:2000, 3:11])
ggpairs(x2[1:2000, 3:11])
ggpairs(x3[1:2000, 3:11])
```

```{r}
# Color Brewer palette
ggplot(x1, aes(xcoord, ycoord, fill= label)) + 
  geom_tile() +
  scale_fill_distiller(palette = "RdPu") +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
              panel.background = element_blank(), axis.line = element_line(colour = "black")) + 
  theme_ipsum() + 
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  theme(panel.grid = element_blank(),
        panel.border = element_blank()) + 
  labs(title = "Map of Labeled Regions in Image 1", x = "X-Coordinate",
       y = "Y-Coordinate")

ggplot(x2, aes(xcoord, ycoord, fill= label)) + 
  geom_tile() +
  scale_fill_distiller(palette = "RdPu") +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
              panel.background = element_blank(), axis.line = element_line(colour = "black")) + 
  theme_ipsum() + 
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  theme(panel.grid = element_blank(),
        panel.border = element_blank()) + 
  labs(title = "Map of Labeled Regions in Image 2", x = "X-Coordinate",
       y = "Y-Coordinate")

ggplot(x3, aes(xcoord, ycoord, fill= label)) + 
  geom_tile() +
  scale_fill_distiller(palette = "RdPu") +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
              panel.background = element_blank(), axis.line = element_line(colour = "black")) + 
  theme_ipsum() + 
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  theme(panel.grid = element_blank(),
        panel.border = element_blank()) + 
  labs(title = "Map of Labeled Regions in Image 3", x = "X-Coordinate",
       y = "Y-Coordinate")
```

Let's start looking for some correlations

```{r}
corrplot.mixed(cor(x1t), order = 'hclust')
corrplot.mixed(cor(x2t), order = 'hclust')
corrplot.mixed(cor(x3t), order = 'hclust')
```

```{r}
p1 <- ggplot(x1t, aes(group = label, NDAI)) +
  geom_boxplot() + 
  theme_ipsum() + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
              panel.background = element_blank(), axis.line = element_line(colour = "black")) + 
  theme(panel.grid = element_blank(),
        panel.border = element_blank()) + 
  labs(title = "NDAI by Label", x = "", y = "") + 
  theme(plot.title = element_text(size=10))

p2 <- ggplot(x1t, aes(group = label, SD)) +
  geom_boxplot() + 
  theme_ipsum() + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
              panel.background = element_blank(), axis.line = element_line(colour = "black")) + 
  theme(panel.grid = element_blank(),
        panel.border = element_blank()) + 
  labs(title = "SD by Label", x = "", y = "") + 
  theme(plot.title = element_text(size=10))

p3 <- ggplot(x1t, aes(group = label, CORR)) +
  geom_boxplot() + 
  theme_ipsum() + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
              panel.background = element_blank(), axis.line = element_line(colour = "black")) + 
  theme(panel.grid = element_blank(),
        panel.border = element_blank()) + 
  labs(title = "CORR by Label", x = "", y = "") + 
  theme(plot.title = element_text(size=10))

p4 <- ggplot(x1t, aes(group = label, DF)) +
  geom_boxplot() + 
  theme_ipsum() + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
              panel.background = element_blank(), axis.line = element_line(colour = "black")) + 
  theme(panel.grid = element_blank(),
        panel.border = element_blank()) + 
  labs(title = "DF by Label", x = "", y = "") + 
  theme(plot.title = element_text(size=10))

p5 <- ggplot(x1t, aes(group = label, CF)) +
  geom_boxplot() + 
  theme_ipsum() + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
              panel.background = element_blank(), axis.line = element_line(colour = "black")) + 
  theme(panel.grid = element_blank(),
        panel.border = element_blank()) + 
  labs(title = "CF by Label", x = "", y = "") + 
  theme(plot.title = element_text(size=10))

p6 <- ggplot(x1t, aes(group = label, BF)) +
  geom_boxplot() + 
  theme_ipsum() + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
              panel.background = element_blank(), axis.line = element_line(colour = "black")) + 
  theme(panel.grid = element_blank(),
        panel.border = element_blank()) + 
  labs(title = "BF by Label", x = "", y = "") + 
  theme(plot.title = element_text(size=3))

p7 <- ggplot(x1t, aes(group = label, AF)) +
  geom_boxplot() + 
  theme_ipsum() + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
              panel.background = element_blank(), axis.line = element_line(colour = "black")) + 
  theme(panel.grid = element_blank(),
        panel.border = element_blank()) + 
  labs(title = "AF by Label", x = "", y = "") + 
  theme(plot.title = element_text(size=10))

p8 <- ggplot(x1t, aes(group = label, AN)) +
  geom_boxplot() + 
  theme_ipsum() + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
              panel.background = element_blank(), axis.line = element_line(colour = "black")) + 
  theme(panel.grid = element_blank(),
        panel.border = element_blank()) + 
  labs(title = "AN by Label", x = "", y = "") + 
  theme(plot.title = element_text(size=10))

grid.arrange(p1, p2, p3, p4, nrow = 2)
grid.arrange(p5, p6, p7, p8, nrow = 2)
```

```{r}
library(plyr)
library(rpart)
set.seed(123)
form <- "cloudy_fac ~ NDAI + CORR + SD + DF + CF + BF + AF + AN"
folds <- split(x1class, cut(sample(1:nrow(x1class)),10))
errs <- rep(NA, length(folds))

for (i in 1:length(folds)) {
 test <- ldply(folds[i], data.frame)
 train <- ldply(folds[-i], data.frame)
 tmp.model <- rpart(form , train, method = "class")
 tmp.predict <- predict(tmp.model, newdata = test, type = "class")
 conf.mat <- table(test$cloudy_fac, tmp.predict)
 errs[i] <- 1-sum(diag(conf.mat))/sum(conf.mat)
}
```

## Question 2

### Part a

Split the data into three sets: training, validation, and test

If we want to split the data, we may want to ensure that we have equal representation from each group, given that they are differentiated in terms of generative behavior. Alternately, we may want to ensure that the points are selected randomly across regions, which we can achieve by breaking the data down into blocks and sampling from each block:

https://arxiv.org/pdf/1906.02899.pdf (paper for reference)

60/20/20 split (training, validation, testing)

```{r}
set.seed(45)
# First, sample 60% from each of the images, then combine
samp1 <- sample(nrow(x1), .6 * nrow(x1))
samp2 <- sample(nrow(x2), .6 * nrow(x2))
samp3 <- sample(nrow(x3), .6 * nrow(x3))

x1_train <- x1[samp1, ]
x2_train <- x2[samp2, ]
x3_train <- x3[samp3, ]

train1 <- rbind(x1_train, x2_train, x3_train)

temp1 <- x1[-samp1, ]
temp2 <- x2[-samp2, ]
temp3 <- x3[-samp3, ]

samp1.2 <- sample(nrow(temp1), .5*nrow(temp1))
samp2.2 <- sample(nrow(temp2), .5*nrow(temp2))
samp3.2 <- sample(nrow(temp2), .5*nrow(temp3))

val1 <- rbind(temp1[samp1.2,], temp2[samp2.2,], temp3[samp3.2,])
test1 <- rbind(temp1[-samp1.2,], temp2[-samp2.2,], temp3[-samp3.2,])
```

```{r}
set.seed(45)
# Second, sample equal rates of each type from each image, then combine
image_1_pos <- x1[x1$label == 1, ]
image_2_pos <- x2[x2$label == 1, ]
image_3_pos <- x3[x3$label == 1, ]

image_1_neg <- x1[x1$label == -1, ]
image_2_neg <- x2[x2$label == -1, ]
image_3_neg <- x3[x3$label == -1, ]

image_1_un <- x1[x1$label == 0, ]
image_2_un <- x2[x2$label == 0, ]
image_3_un <- x3[x3$label == 0, ]

# First split up each of the postiive
indices_1 <- sample(rep(1:3, times = c(.6 * nrow(image_1_pos), .2 * nrow(image_1_pos), 
                                       .2 * nrow(image_1_pos))))
pos_1_split <- split(image_1_pos, indices_1)

indices_2 <- sample(rep(1:3, times = c(.6 * nrow(image_2_pos), .2 * nrow(image_2_pos), 
                                       .2 * nrow(image_2_pos))))
pos_2_split <- split(image_2_pos, indices_2)

indices_3 <- sample(rep(1:3, times = c(.6 * nrow(image_3_pos), .2 * nrow(image_3_pos), 
                                       .2 * nrow(image_3_pos))))
pos_3_split <- split(image_3_pos, indices_3)

# Now do the same for negative
indices_1 <- sample(rep(1:3, times = c(.6 * nrow(image_1_neg), .2 * nrow(image_1_neg), 
                                       .2 * nrow(image_1_neg))))
neg_1_split <- split(image_1_neg, indices_1)

indices_2 <- sample(rep(1:3, times = c(.6 * nrow(image_2_neg), .2 * nrow(image_2_neg), 
                                       .2 * nrow(image_2_neg))))
neg_2_split <- split(image_2_neg, indices_2)

indices_3 <- sample(rep(1:3, times = c(.6 * nrow(image_3_neg), .2 * nrow(image_3_neg), 
                                       .2 * nrow(image_3_neg))))
neg_3_split <- split(image_3_neg, indices_3)

# Finally, split across the non-labeled points
indices_1 <- sample(rep(1:3, times = c(.6 * nrow(image_1_un), .2 * nrow(image_1_un), 
                                       .2 * nrow(image_1_un))))
un_1_split <- split(image_1_un, indices_1)

indices_2 <- sample(rep(1:3, times = c(.6 * nrow(image_2_un), .2 * nrow(image_2_un), 
                                       .2 * nrow(image_2_un))))
un_2_split <- split(image_2_neg, indices_2)

indices_3 <- sample(rep(1:3, times = c(.6 * nrow(image_3_un), .2 * nrow(image_3_un), 
                                       .2 * nrow(image_3_un))))
un_3_split <- split(image_3_un, indices_3)

# Then combine
train2 <- rbind(pos_1_split$`1`, pos_2_split$`1`, pos_3_split$`1`,
                neg_1_split$`1`, neg_1_split$`1`, neg_1_split$`1`,
                un_1_split$`1`, un_1_split$`1`, un_1_split$`1`)

val2 <- rbind(pos_1_split$`2`, pos_2_split$`2`, pos_3_split$`2`,
              neg_1_split$`2`, neg_1_split$`2`, neg_1_split$`2`,
              un_1_split$`2`, un_1_split$`2`, un_1_split$`2`)

test2 <- rbind(pos_1_split$`3`, pos_2_split$`3`, pos_3_split$`3`,
               neg_1_split$`3`, neg_1_split$`3`, neg_1_split$`3`,
               un_1_split$`3`, un_1_split$`3`, un_1_split$`3`)
```

### Part b

Report the accuracy of a trivial classifier setting all labels to -1

```{r}
mean(train2$label == 0)
mean(test2$label == 0)
mean(val2$label == 0)
```
This will perform especially well in the case where most of the points share the label of the naive estimate.

### Part c

Important features: NDAI, CORR, and AN (at least for two of the images)

### Part d

```{r}
class_accuracy <- function(xfeatures, xlabels) {
  return(sum(diag(table(xfeatures, xlabels))) / length(xlabels))
}

CVmaster <- function(classifier, features, labels, 
                     kfolds = 5, loss = class_accuracy) {
  # Iterate over each of the folds
  # Start by splitting the data
  indices <- sample(rep(1:kfolds, times = rep(1/kfolds * nrow(xfeatures), kfolds)))
  split_features <- split(features, indices)
  split_labels <- split(labels, indices)
  
  accuracies <- rep(NA, kfolds)
  
  for (i in 1:kfolds) {
    feat <- split_features[[i]]
    labs <- split_labels[[i]]
    
    classifier_labels <- classifier(feat, labs)
    accuracy <- loss(labs, classifier_labels)
    
    accuracies[i] <- accuracy
  }
  
  return(mean(accuracies))
}
```


## Part 4

Let's perform SVM

```{r}
xt <- rbind(x1t, x2t, x3t)

samps <-sample(nrow(xt), .8*nrow(xt))
xt_train <- xt[samps, ]
xt_test <- xt[-samps, ]
xt_train_labels <- xt_train$label
xt_train <- xt_train[, names(xt_train) != "label"]
xt_test_labels <- xt_test$label
xt_test <- xt_test[, names(xt_test) != "label"]

# 
# ctrl <- trainControl(method = "repeatedcv", repeats = 5, savePredictions = TRUE, 
#                      classProbs = TRUE)
# 
# svm.tune <- train(x = xt_train, y = xt_train_labels, method = "svmLinear", 
#                   tuneLength = 5, preProcess = c("center", "scale"), trControl = ctrl,
#                   ranges = list(gamma = seq(0,.2,.01), cost = 2^(2:9)),
#                   tunecon)
# 
# svm.tune
# 
# plot(svm.tune)


# x1t$label <- as.factor(x1t$label)
tune_model <- tune(svm, label ~ ., data = xt, 
          ranges = list(gamma = 2^(-3:3), cost = 2^(2:9)),
          tunecontrol = tune.control(sampling = "fix"))

summary(m2)
m2$SV

plot(m2, data = x1t, xcoord ~ ycoord)

tunedModel <- svm.tune$bestTune
tunedModelY <- predict(tunedModel, xt_test)
error <- svm.tune$best.model$residuals

tunedModelRMSE <- rmse(error)
tunedModelRMSE
```

```{r}
plot(m2, data = df)
```


## Question 3

# Logistic Regression
```{r}
set.seed(45)
# First Method Data
method1_data <- rbind(train1, val1)

method1_class <- method1_data %>%
  filter(label == -1 | label == 1) %>%
  mutate(cloudy_fac = NA)

method1_class$cloudy_fac <- factor(method1_class$label, labels = c("not_cloudy", "cloudy"))

method1_test <- test1 %>%
  filter(label == -1 | label == 1) %>%
  mutate(cloudy_fac = NA)

method1_test$cloudy_fac <- factor(method1_test$label, labels = c("not_cloudy", "cloudy"))

# Second Method Data
method2_data <- rbind(train2, val2)

method2_class <- method2_data %>%
  filter(label == -1 | label == 1) %>%
  mutate(cloudy_fac = NA)
  
method2_class$cloudy_fac <- factor(method2_class$label, labels = c("not_cloudy", "cloudy"))

method2_test <- test2 %>%
  filter(label == -1 | label == 1) %>%
  mutate(cloudy_fac = NA)

method2_test$cloudy_fac <- factor(method2_test$label, labels = c("not_cloudy", "cloudy"))

# Cross Validation trainControl
cv_train <- trainControl(method = "cv", number = 5, savePredictions = T)

# Models
m1logistic_model <- train(cloudy_fac ~ NDAI + CORR + SD + DF + CF + BF + AF + AN,
               data = method1_class,
               trControl = cv_train,
               method = "glm",
               family=binomial())

m2logistic_model <- train(cloudy_fac ~ NDAI + CORR + SD + DF + CF + BF + AF + AN,
               data = method2_class,
               trControl = cv_train,
               method = "glm",
               family=binomial())

# ROC Curves
predm1_logit <- predict(m1logistic_model, newdata=method1_test, type = "raw")
ROCRpredm1_logit <- prediction(as.numeric(predm1_logit),as.numeric(method1_test$cloudy_fac))
ROCRperfm1_logit <- performance(ROCRpredm1_logit, 'tpr', 'fpr')

predm2_logit <- predict(m2logistic_model, newdata=method2_test, type = "raw")
ROCRpredm2_logit <- prediction(as.numeric(predm2_logit),as.numeric(method2_test$cloudy_fac))
ROCRperfm2_logit <- performance(ROCRpredm2_logit, 'tpr', 'fpr')

plot(ROCRperfm1_logit, col = "red4", text.adj=c(-0.2,1.7))
plot(ROCRperfm2_logit, col = "red", text.adj=c(-0.2,1.7), add = T)

# Fold and test accuracy
logitm1_fold_avg <- m1logistic_model$pred %>%
  group_by(Resample) %>%
  summarise(accuracy = mean(pred == obs))

logitm2_fold_avg <- m2logistic_model$pred %>%
  group_by(Resample) %>%
  summarise(accuracy = mean(pred == obs))

confusionMatrix(data=predm1_logit, method1_test$cloudy_fac)
confusionMatrix(data=predm2_logit, method2_test$cloudy_fac)
```

# KNN
```{r}
# Models
m1KNN_model <- train(cloudy_fac ~ NDAI + CORR + SD + DF + CF + BF + AF + AN,
             method = "knn",
             tuneGrid = expand.grid(k = 1:10),
             trControl = cv_train,
             metric = "Accuracy",
             data = method1_class)

m2KNN_model <- train(cloudy_fac ~ NDAI + CORR + SD + DF + CF + BF + AF + AN,
             method = "knn",
             tuneGrid = expand.grid(k = 1:10),
             trControl = cv_train,
             metric = "Accuracy",
             data = method2_class)

# ROC Curves
predm1_knn <- predict(m1KNN_model, newdata=method1_test, type = "raw")
ROCRpredm1_knn <- prediction(as.numeric(predm1_knn),as.numeric(method1_test$cloudy_fac))
ROCRperfm1_knn <- performance(ROCRpredm1_knn, 'tpr', 'fpr')

predm2_knn <- predict(m2KNN_model, newdata=method2_test, type = "raw")
ROCRpredm2_knn <- prediction(as.numeric(predm2_knn),as.numeric(method2_test$cloudy_fac))
ROCRperfm2_knn <- performance(ROCRpredm2_knn, 'tpr', 'fpr')

plot(ROCRperfm1_knn, col = "blue4", text.adj=c(-0.2,1.7))
plot(ROCRperfm2_knn, col = "blue", text.adj=c(-0.2,1.7), add = T)

# Fold and test accuracy
knnm1_fold_avg <- m1KNN_model$pred %>%
  group_by(Resample) %>%
  summarise(accuracy = mean(pred == obs))

knnm2_fold_avg <- m2KNN_model$pred %>%
  group_by(Resample) %>%
  summarise(accuracy = mean(pred == obs))

confusionMatrix(data=predm1_knn, method1_test$cloudy_fac)
confusionMatrix(data=predm2_knn, method2_test$cloudy_fac)
```

# SVM
```{r}
# Models
m1svm_model <- train(cloudy_fac ~ NDAI + CORR + SD + DF + CF + BF + AF + AN, 
                     data = method1_class, 
                     method = "svmLinear", 
                     trControl = cv_train)
m2svm_model <- train(cloudy_fac ~ NDAI + CORR + SD + DF + CF + BF + AF + AN, 
                     data = method2_class, 
                     method = "svmLinear", 
                     trControl = cv_train)

# ROC Curves
predm1_svm <- predict(m1svm_model, newdata=method1_test, type = "raw")
ROCRpredm1_svm <- prediction(as.numeric(predm1_svm),as.numeric(method1_test$cloudy_fac))
ROCRperfm1_svm <- performance(ROCRpredm1_svm, 'tpr', 'fpr')

predm2_svm <- predict(m2svm_model, newdata=method2_test, type = "raw")
ROCRpredm2_svm <- prediction(as.numeric(predm2_svm),as.numeric(method2_test$cloudy_fac))
ROCRperfm2_svm <- performance(ROCRpredm2_svm, 'tpr', 'fpr')

plot(ROCRperfm1_svm, col = "green4", text.adj=c(-0.2,1.7))
plot(ROCRperfm2_svm, col = "green", text.adj=c(-0.2,1.7), add = T)

# Fold and test accuracy
svmm1_fold_avg <- m1svm_model$pred %>%
  group_by(Resample) %>%
  summarise(accuracy = mean(pred == obs))

svmm2_fold_avg <- m2svm_model$pred %>%
  group_by(Resample) %>%
  summarise(accuracy = mean(pred == obs))

confusionMatrix(data=predm1_svm, method1_test$cloudy_fac)
confusionMatrix(data=predm2_svm, method2_test$cloudy_fac)
```

# Random Forest
```{r}
# Models
m1rf_model <- train(cloudy_fac ~ NDAI + CORR + SD + DF + CF + BF + AF + AN, 
                      data=method1_class, 
                      method='rf', 
                      metric='Accuracy', 
                      trControl=cv_train)

m2rf_model <- train(cloudy_fac ~ NDAI + CORR + SD + DF + CF + BF + AF + AN, 
                      data=method2_class, 
                      method='rf', 
                      metric='Accuracy', 
                      trControl=cv_train)

# ROC Curves
predm1_rf <- predict(m1svm_model, newdata=method1_test, type = "raw")
ROCRpredm1_rf <- prediction(as.numeric(predm1_rf),as.numeric(method1_test$cloudy_fac))
ROCRperfm1_rf <- performance(ROCRpredm1_rf, 'tpr', 'fpr')

predm2_rf <- predict(m2rf_model, newdata=method2_test, type = "raw")
ROCRpredm2_rf <- prediction(as.numeric(predm2_rf),as.numeric(method2_test$cloudy_fac))
ROCRperfm2_rf <- performance(ROCRpredm2_rf, 'tpr', 'fpr')

plot(ROCRperfm1_rf, col = "black", text.adj=c(-0.2,1.7))
plot(ROCRperfm2_rf, col = "gray", text.adj=c(-0.2,1.7), add = T)

# Fold and test accuracy
rfm1_fold_avg <- m1rf_model$pred %>%
  group_by(Resample) %>%
  summarise(accuracy = mean(pred == obs))

rfm2_fold_avg <- m2rf_model$pred %>%
  group_by(Resample) %>%
  summarise(accuracy = mean(pred == obs))

confusionMatrix(data=predm1_rf, method1_test$cloudy_fac)
confusionMatrix(data=predm2_rf, method2_test$cloudy_fac)
```

# All ROC Curves
```{r}
plot(ROCRperfm1_logit, col = "red4", text.adj=c(-0.2,1.7), main = "ROC Curves", lwd = 2,
     print.cutoffs.at = .5)
plot(ROCRperfm2_logit, col = "red", text.adj=c(-0.2,1.7), add = T, lwd = 2)
plot(ROCRperfm1_knn, col = "blue4", text.adj=c(-0.2,1.7), add = T, lwd = 2)
plot(ROCRperfm2_knn, col = "blue", text.adj=c(-0.2,1.7), add = T, lwd = 2)
plot(ROCRperfm1_svm, col = "green4", text.adj=c(-0.2,1.7), add = T, lwd = 2)
plot(ROCRperfm2_svm, col = "green", text.adj=c(-0.2,1.7), add = T, lwd = 2)
plot(ROCRperfm1_rf, col = "black", text.adj=c(-0.2,1.7), add = T, lwd = 2)
plot(ROCRperfm2_rf, col = "gray", text.adj=c(-0.2,1.7), add = T, lwd = 2)
legend(x = "bottomright",          
       legend = c("M1 Logit", "M2 Logit", "M1 KNN", "M2 KNN", "M1 SVM", "M2 SVM", "M1 Random Forest",
                  "M2 Random Forest"), 
       lty = c(1, 1, 1, 1, 1, 1, 1, 1),
       lwd = c(2, 2, 2, 2, 2, 2, 2, 2),
       col = c("red4","red", "blue4", "blue", "green4", "green", "black", "gray"))

```
# Cut point 
```{r}
cut_point = function(perf, pred){
    cut.ind = mapply(FUN=function(x, y, p){
        d = (x - 0)^2 + (y-1)^2
        ind = which(d == min(d))
        c(sensitivity = y[[ind]], specificity = 1-x[[ind]], 
            cutoff = p[[ind]])
    }, perf@x.values, perf@y.values, pred@cutoffs)
}



cut_point(ROCRperfm2_logit, ROCRpredm2_logit)[,1]
cut_point(ROCRperfm2_knn, ROCRpredm2_knn)[,1]
cut_point(ROCRperfm2_svm, ROCRpredm2_svm)[,1]
cut_point(ROCRperfm2_rf, ROCRpredm2_rf)[,1]
```


